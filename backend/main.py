import os
import sys
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv
import logging
from datetime import datetime
import requests
from io import BytesIO
import traceback

# ============================================
# CONFIGURACI√ìN HF_TOKEN
# ============================================
# Cargar variables de entorno
load_dotenv()

# Obtener HF_TOKEN - IMPORTANTE para modelos privados o rate limits
HF_TOKEN = os.getenv("HF_TOKEN", "")

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Mostrar info del token (sin mostrar el valor completo por seguridad)
if HF_TOKEN:
    logger.info(f"‚úÖ HF_TOKEN configurado ({len(HF_TOKEN)} caracteres)")
    # Configurar header para requests a Hugging Face
    HF_HEADERS = {"Authorization": f"Bearer {HF_TOKEN}"}
else:
    logger.warning("‚ö†Ô∏è  HF_TOKEN no configurado. Algunos modelos pueden tener rate limits.")
    logger.info("üí° Obt√©n un token gratis en: https://huggingface.co/settings/tokens")
    HF_HEADERS = {}

# ============================================
# CONFIGURACI√ìN DEL MODELO
# ============================================
MODEL_NAME = os.getenv("LOCAL_MODEL", "google/vit-base-patch16-224")

# ============================================
# IMPORTAR MODELO DE HUGGING FACE
# ============================================
try:
    import torch
    from PIL import Image
    from transformers import AutoImageProcessor, AutoModelForImageClassification
    import torch.nn.functional as F
    
    MODEL_SUPPORT = True
    MODEL_CACHE = {}
    
    def cargar_modelo():
        """Carga el modelo usando HF_TOKEN si est√° disponible"""
        if not MODEL_SUPPORT:
            raise RuntimeError("Librer√≠as de ML no instaladas")
        
        if "model" not in MODEL_CACHE:
            logger.info(f"ü§ñ Cargando modelo: {MODEL_NAME}")
            
            try:
                # Configurar token para la descarga
                kwargs = {}
                if HF_TOKEN:
                    kwargs["token"] = HF_TOKEN
                    kwargs["use_auth_token"] = HF_TOKEN
                
                # Cargar procesador y modelo
                processor = AutoImageProcessor.from_pretrained(MODEL_NAME, **kwargs)
                model = AutoModelForImageClassification.from_pretrained(MODEL_NAME, **kwargs)
                
                model.eval()
                
                MODEL_CACHE["model"] = model
                MODEL_CACHE["processor"] = processor
                
                logger.info(f"‚úÖ Modelo '{MODEL_NAME}' cargado exitosamente")
                logger.info(f"   Dispositivo: {'GPU' if torch.cuda.is_available() else 'CPU'}")
                
            except Exception as e:
                logger.error(f"‚ùå Error cargando modelo: {e}")
                
                # Fallback a un modelo m√°s peque√±o y p√∫blico
                logger.info("üîÑ Intentando con modelo alternativo p√∫blico...")
                try:
                    MODEL_FALLBACK = "google/vit-base-patch16-224"
                    processor = AutoImageProcessor.from_pretrained(MODEL_FALLBACK)
                    model = AutoModelForImageClassification.from_pretrained(MODEL_FALLBACK)
                    model.eval()
                    
                    MODEL_CACHE["model"] = model
                    MODEL_CACHE["processor"] = processor
                    logger.info(f"‚úÖ Modelo alternativo '{MODEL_FALLBACK}' cargado")
                except Exception as e2:
                    logger.error(f"‚ùå Error con modelo alternativo: {e2}")
                    raise RuntimeError(f"No se pudo cargar ning√∫n modelo")
        
        return MODEL_CACHE["processor"], MODEL_CACHE["model"]
    
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è  No se pudieron importar librer√≠as de ML: {e}")
    logger.warning("   Ejecutando en modo mock")
    MODEL_SUPPORT = False

# ============================================
# FASTAPI APP
# ============================================
app = FastAPI(
    title="API de Etiquetado de Im√°genes",
    description="Clasificaci√≥n de im√°genes usando Hugging Face",
    version="1.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class URLItem(BaseModel):
    url: str

# ============================================
# ENDPOINTS
# ============================================
@app.get("/")
def home():
    return {
        "status": "ok",
        "message": "‚úÖ Backend funcionando",
        "model": MODEL_NAME,
        "mode": "real_inference" if MODEL_SUPPORT else "mock_mode",
        "hf_token": "configured" if HF_TOKEN else "not_configured",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/health")
def health():
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "services": {
            "model": "ready" if MODEL_CACHE else "not_loaded",
            "hf_token": "configured" if HF_TOKEN else "not_configured",
            "api": "operational"
        }
    }

@app.post("/etiquetar")
async def etiquetar(item: URLItem):
    """Endpoint principal para etiquetar im√°genes"""
    start_time = datetime.now()
    
    try:
        logger.info(f"üì® Procesando imagen: {item.url[:100]}...")
        
        # 1. Validar URL
        if not item.url.startswith(('http://', 'https://')):
            raise HTTPException(status_code=400, detail="URL debe comenzar con http:// o https://")
        
        # 2. Descargar imagen
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(item.url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # 3. Verificar que sea imagen
        content_type = response.headers.get('content-type', '')
        if 'image' not in content_type:
            raise HTTPException(status_code=400, detail=f"URL no apunta a una imagen. Content-Type: {content_type}")
        
        # 4. Abrir imagen
        image = Image.open(BytesIO(response.content)).convert("RGB")
        
        # 5. Procesar con modelo
        if MODEL_SUPPORT:
            try:
                # Cargar modelo
                processor, model = cargar_modelo()
                
                # Procesar imagen
                inputs = processor(images=image, return_tensors="pt")
                
                # Inferencia
                with torch.no_grad():
                    outputs = model(**inputs)
                    logits = outputs.logits
                    probabilities = F.softmax(logits, dim=-1)
                
                # Obtener top 5 predicciones
                top_probs, top_indices = torch.topk(probabilities, k=5)
                
                etiquetas = []
                for i in range(5):
                    idx = top_indices[0][i].item()
                    score = top_probs[0][i].item()
                    
                    # Obtener etiqueta legible
                    if hasattr(model.config, 'id2label') and idx in model.config.id2label:
                        label = model.config.id2label[idx]
                    else:
                        label = f"clase_{idx}"
                    
                    etiquetas.append({
                        "label": label,
                        "score": round(score, 4),
                        "percentage": round(score * 100, 2)
                    })
                
                mode = "real_inference"
                
            except Exception as e:
                logger.error(f"‚ùå Error en inferencia: {e}")
                # Fallback a modo mock
                etiquetas = get_mock_predictions()
                mode = "mock_fallback"
                
        else:
            # Modo mock si no hay soporte de ML
            etiquetas = get_mock_predictions()
            mode = "mock_mode"
        
        # 6. Calcular tiempo
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # 7. Preparar respuesta
        response_data = {
            "status": "success",
            "url": item.url,
            "etiquetas": etiquetas,
            "modelo": MODEL_NAME,
            "tiempo_procesamiento": processing_time,
            "timestamp": datetime.now().isoformat(),
            "mode": mode,
            "hf_token_used": bool(HF_TOKEN)
        }
        
        # 8. Log resultados
        logger.info(f"üè∑Ô∏è  Etiquetas generadas: {len(etiquetas)}")
        logger.info(f"‚è±Ô∏è  Tiempo: {processing_time:.2f}s")
        logger.info(f"üîß Modo: {mode}")
        
        return response_data
        
    except requests.exceptions.RequestException as e:
        raise HTTPException(status_code=400, detail=f"Error descargando imagen: {e}")
    except Exception as e:
        logger.error(f"‚ùå Error inesperado: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error procesando imagen: {str(e)}")

def get_mock_predictions():
    """Predicciones mock para cuando el modelo no est√° disponible"""
    return [
        {"label": "computer_keyboard", "score": 0.95, "percentage": 95.0},
        {"label": "mouse", "score": 0.03, "percentage": 3.0},
        {"label": "monitor", "score": 0.02, "percentage": 2.0},
        {"label": "laptop", "score": 0.01, "percentage": 1.0},
        {"label": "desktop_computer", "score": 0.01, "percentage": 1.0}
    ]

@app.get("/model/info")
def get_model_info():
    """Informaci√≥n del modelo"""
    if MODEL_SUPPORT and "model" in MODEL_CACHE:
        model = MODEL_CACHE["model"]
        return {
            "modelo": MODEL_NAME,
            "tipo": type(model).__name__,
            "num_clases": model.config.num_labels,
            "estado": "cargado",
            "hf_token": "si" if HF_TOKEN else "no"
        }
    else:
        return {
            "modelo": MODEL_NAME,
            "estado": "no_cargado" if MODEL_SUPPORT else "no_soportado",
            "modo": "mock",
            "hf_token": "si" if HF_TOKEN else "no",
            "mensaje": "Usa /etiquetar para probar la API"
        }

# ============================================
# INICIALIZACI√ìN
# ============================================
@app.on_event("startup")
async def startup_event():
    """Carga el modelo al iniciar"""
    logger.info("=" * 70)
    logger.info("üöÄ INICIANDO ETIQUETADOR DE IM√ÅGENES")
    logger.info(f"ü§ñ Modelo: {MODEL_NAME}")
    logger.info(f"üîë HF_TOKEN: {'‚úÖ Configurado' if HF_TOKEN else '‚ö†Ô∏è  No configurado'}")
    logger.info(f"‚ö° ML Support: {'‚úÖ Disponible' if MODEL_SUPPORT else '‚ö†Ô∏è  No disponible'}")
    logger.info("=" * 70)
    
    if MODEL_SUPPORT:
        try:
            # Carga diferida - se cargar√° con la primera solicitud
            logger.info("üì¶ El modelo se cargar√° bajo demanda")
        except Exception as e:
            logger.error(f"‚ùå Error inicializando modelo: {e}")
    else:
        logger.info("üé≠ Ejecutando en modo mock")

# ============================================
# EJECUCI√ìN
# ============================================
if __name__ == "__main__":
    import uvicorn
    
    port = int(os.getenv("PORT", 10000))
    
    logger.info(f"üåç Servidor iniciando en puerto {port}")
    logger.info(f"üìö API Docs: http://localhost:{port}/docs")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=port,
        log_level="info"
    )